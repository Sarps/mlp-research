{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.217857Z",
     "start_time": "2024-02-19T14:41:37.170645Z"
    }
   },
   "source": [
    "import os\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "\n",
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.221508Z",
     "start_time": "2024-02-19T14:41:37.218943Z"
    }
   },
   "cell_type": "code",
   "source": "path_to_file",
   "id": "75edb612ae9bbae8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emmanuelsarpong/.keras/datasets/spa-eng/spa.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.225332Z",
     "start_time": "2024-02-19T14:41:37.222216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = ' ' + w + ' '\n",
    "    return w"
   ],
   "id": "c0a28001d75b8381",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.229593Z",
     "start_time": "2024-02-19T14:41:37.226837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from archive.lib.language_index import LanguageIndex\n",
    "\n",
    "def create_dataset(path: str, num_examples: int) -> tuple[List[str], List[str]]:\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(sentence) for sentence in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    return [inp for inp, targ in word_pairs], [targ for inp, targ in word_pairs]\n",
    "\n",
    "def load_dataset(path: str, num_examples: int):\n",
    "    en, sp = create_dataset(path, num_examples)\n",
    "    \n",
    "    return LanguageIndex([s.split(' ') for s in sp]), LanguageIndex([e.split(' ') for e in en])"
   ],
   "id": "11df0a47f8ab479d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.363571Z",
     "start_time": "2024-02-19T14:41:37.230478Z"
    }
   },
   "cell_type": "code",
   "source": "inp_lang, targ_lang = load_dataset(path_to_file, 10000)",
   "id": "7c9ba8f9c5bee8b9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.398456Z",
     "start_time": "2024-02-19T14:41:37.364391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
    "    inp_lang.tensor(), \n",
    "    targ_lang.tensor(),\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ],
   "id": "507d1c63638e31ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000, 2000, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.402189Z",
     "start_time": "2024-02-19T14:41:37.399717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import Model, layers\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = layers.GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ],
   "id": "aa41a722b37c654b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.406260Z",
     "start_time": "2024-02-19T14:41:37.402760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = layers.GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ],
   "id": "20d4eae330b14225",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.416692Z",
     "start_time": "2024-02-19T14:41:37.407043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "encoder = Encoder(vocab_size=len(inp_lang.word2idx) + 1, embedding_dim=embedding_dim, enc_units=units, batch_sz=BATCH_SIZE)\n",
    "decoder = Decoder(len(targ_lang.word2idx) + 1, embedding_dim, units, BATCH_SIZE)"
   ],
   "id": "b1725bbcdeb99996",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.419263Z",
     "start_time": "2024-02-19T14:41:37.417298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "optimizer = optimizers.legacy.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "id": "bb56ab6060d7721f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.421652Z",
     "start_time": "2024-02-19T14:41:37.419768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ],
   "id": "5bcab7aa80a19a63",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:43:59.740931Z",
     "start_time": "2024-02-19T14:43:59.631094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import Model, layers, Input\n",
    "import tensorflow as tf\n",
    "\n",
    "class Seq2Seq(Model):\n",
    "    def __init__(self, encoder, decoder, batch_sz):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.batch_sz = batch_sz\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        print(inputs)\n",
    "        inp, targ = inputs\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.zero_idx] * self.batch_sz, 1)\n",
    "\n",
    "        # Assuming teacher forcing in training mode\n",
    "        if training:\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            return predictions\n",
    "        else:\n",
    "            # Implement inference mode logic if necessary\n",
    "            pass\n",
    "\n",
    "# Model compilation and training\n",
    "model = Seq2Seq(encoder, decoder, batch_sz=BATCH_SIZE)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=loss_function)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(input_tensor_train, target_tensor_train, epochs=10)\n"
   ],
   "id": "98e9a498b324b96b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Tensor(\"IteratorGetNext:0\", shape=(32, 12), dtype=int32)\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'seq2_seq_4' (type Seq2Seq).\n    \n    in user code:\n    \n        File \"/var/folders/fk/6lvr7z756yjcvwrjfwxbtl8c0000gp/T/ipykernel_62255/4275040482.py\", line 13, in call  *\n            inp, targ = inputs\n    \n        OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n    \n    \n    Call arguments received by layer 'seq2_seq_4' (type Seq2Seq):\n      • inputs=tf.Tensor(shape=(32, 12), dtype=int32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOperatorNotAllowedInGraphError\u001B[0m            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 37\u001B[0m\n\u001B[1;32m     34\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39mloss_function)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(input_tensor_train, target_tensor_train, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001B[0m, in \u001B[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m     51\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m---> 52\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m     53\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mOperatorNotAllowedInGraphError\u001B[0m: in user code:\n\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'seq2_seq_4' (type Seq2Seq).\n    \n    in user code:\n    \n        File \"/var/folders/fk/6lvr7z756yjcvwrjfwxbtl8c0000gp/T/ipykernel_62255/4275040482.py\", line 13, in call  *\n            inp, targ = inputs\n    \n        OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\n    \n    \n    Call arguments received by layer 'seq2_seq_4' (type Seq2Seq):\n      • inputs=tf.Tensor(shape=(32, 12), dtype=int32)\n      • training=True\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.473691Z",
     "start_time": "2024-02-19T14:41:37.473631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang):\n",
    "    attention_plot = np.zeros((targ_lang.max_length, inp_lang.max_length))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=inp_lang.max_length, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['']], 0)\n",
    "\n",
    "    for t in range(targ_lang.max_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ],
   "id": "c2fab4a99b008d4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T14:41:37.474506Z",
     "start_time": "2024-02-19T14:41:37.474313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='Greys')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "cbda2229439f1181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang)\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ],
   "id": "56925a34ae6f4329",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "checkpoint_prefix",
   "id": "af3b3c87294414b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "checkpoint_dir",
   "id": "1904fb2ad431618f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "translate(u'tengo hambre', encoder, decoder, inp_lang, targ_lang)",
   "id": "48c6ea00a20bcf8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "translate(u'tengo', encoder, decoder, inp_lang, targ_lang)",
   "id": "548117097db7b316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "encoder.variables",
   "id": "91a2c55348afe1e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "471cda6a9f281bf5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
