{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T11:14:47.943085Z",
     "start_time": "2024-02-25T11:14:44.979901Z"
    }
   },
   "cell_type": "code",
   "source": "from keras import layers, models",
   "id": "d40e6358f2f2055",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Custom Attention Layers",
   "id": "b16e7374b55edee5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseAttention(layers.Layer):\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.mha = layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_vector, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
    "        self.last_attn_scores = attn_scores\n",
    "        x = self.add([x, attn_vector])\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "class SelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_vector = self.mha(query=x, value=x, key=x)\n",
    "        x = self.add([x, attn_vector])\n",
    "        return self.layernorm(x)\n",
    "\n",
    "\n",
    "class MaskedSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        return self.layernorm(x)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seq = models.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        return self.layer_norm(x)"
   ],
   "id": "de2562a7c9792b9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
