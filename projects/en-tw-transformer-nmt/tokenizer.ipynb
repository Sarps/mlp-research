{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:44.072527Z",
     "start_time": "2024-02-27T20:54:44.070263Z"
    }
   },
   "cell_type": "code",
   "source": "asset_dir = '../../data/en-tw'",
   "id": "f0cf9660eddff12a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:44.086012Z",
     "start_time": "2024-02-27T20:54:44.074380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "asset_dir = os.getcwd() + '/' + asset_dir\n",
    "os.makedirs(asset_dir, exist_ok=True)"
   ],
   "id": "d798aec7889310f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:44.088855Z",
     "start_time": "2024-02-27T20:54:44.087179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from archive.lib.loaders import __download_and_extract, __read_lines\n",
    "from tokenizers.implementations import CharBPETokenizer, BertWordPieceTokenizer"
   ],
   "id": "a0254b5ced058f62",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:45.847134Z",
     "start_time": "2024-02-27T20:54:44.089444Z"
    }
   },
   "cell_type": "code",
   "source": "tw_path, en_path = __download_and_extract('https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/en-tw.txt.zip', 'NLLB.en-tw.tw', 'NLLB.en-tw.en')",
   "id": "a24be62323ec9f5f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:45.850180Z",
     "start_time": "2024-02-27T20:54:45.848422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('[^a-zA-Z0-9ɛƐɔƆ\\\\s!\"#$%&\\\\\\'()*+,\\\\-./:;<=>?@\\\\[\\\\]\\\\^_`{|}~]')"
   ],
   "id": "b7d7a01ecb702b05",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:49.979431Z",
     "start_time": "2024-02-27T20:54:45.850817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# num_lines_to_extract = 1_000_000\n",
    "# \n",
    "# with open(tw_path, 'r') as original_file:\n",
    "#     lines = [next(original_file) for _ in range(num_lines_to_extract)]\n",
    "# \n",
    "# with open('extracted_lines.txt', 'w') as new_file:\n",
    "#     new_file.writelines(lines)\n",
    "# \n",
    "# tw_path = 'extracted_lines.txt'\n",
    "\n",
    "written_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "with open(en_path, 'r', encoding='utf-8') as english_file, \\\n",
    "        open(tw_path, 'r', encoding='utf-8') as twi_file, \\\n",
    "        open(f'{asset_dir}/en.txt', 'w', encoding='utf-8') as clean_english_file, \\\n",
    "        open(f'{asset_dir}/tw.txt', 'w', encoding='utf-8') as clean_twi_file:\n",
    "\n",
    "    for eng_line, twi_line in zip(english_file, twi_file):\n",
    "        if pattern.search(twi_line) is None and pattern.search(eng_line) is None:\n",
    "            written_count += 1\n",
    "            clean_english_file.write(eng_line)\n",
    "            clean_twi_file.write(twi_line)\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "print(written_count, skipped_count)"
   ],
   "id": "234abd29ab09057c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3593806 443408\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:49.981715Z",
     "start_time": "2024-02-27T20:54:49.980228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %memit\n",
    "# %time\n",
    "# \n",
    "# english = BertWordPieceTokenizer()\n",
    "# english.train(['../../data/en-tw/en.txt'], vocab_size=30000)\n",
    "# english.save('../../data/en-tw/en.vocabs.json')"
   ],
   "id": "e9f1914e8db6f4c8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:49.987486Z",
     "start_time": "2024-02-27T20:54:49.982265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %memit\n",
    "# %time\n",
    "# \n",
    "# twi = BertWordPieceTokenizer()\n",
    "# twi.train(['../../data/en-tw/tw.txt'], vocab_size=15000)\n",
    "# twi.save('../../data/en-tw/tw.vocabs.json')"
   ],
   "id": "88a269ee1123d645",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:50.003312Z",
     "start_time": "2024-02-27T20:54:49.988025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(twi.encode(\"sɛ wo dɔ w'awurade in your heart, wo koma mu ne w'ade nyinaa mu\").tokens)\n",
    "# print(twxai.encode('kofi manu ka kyerɛɛ ama ataa sɛ ɔrekɔ kumasi aba but yɛ pɛ no wurawurafoɔ enti boys no moabrɛ a, montua toɔ').tokens)"
   ],
   "id": "46c7dc97547e9863",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:50.007964Z",
     "start_time": "2024-02-27T20:54:50.003805Z"
    }
   },
   "cell_type": "code",
   "source": "# print(english.encode(\"Like fire in the sky, I soar above discombabulations and instrusions\").ids)\n",
   "id": "6df1d399437c00dc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:54:50.010969Z",
     "start_time": "2024-02-27T20:54:50.009716Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "30eba41ac59d9eaa",
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
